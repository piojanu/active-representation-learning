algo: "PPO"
loss:
  clip_ratio_pi: 0.1 # Parameter for clipping the policy objective
  clip_ratio_vf: null # Parameter for clipping the value objective (null to disable)
  entropy_coef: 0.0 # Entropy term coefficient
  value_coef: 0.5 # Value loss coefficient
model:
  recurrent: True # Whether to use a recurrent policy
  hidden_size: 256 # Size of all hidden dense layers
training:
  learning_rate: 3e-4 # Learning rate
  num_epochs: 10 # Numbers of PPO training iterations
  max_grad_norm: 0.5 # Max norm of gradients
  max_kl: 0.015 # Stop PPO epoch if KL divergence between old and new policies exceeds this value
  mini_batch_size: 256 # PPO update mini-batch size
