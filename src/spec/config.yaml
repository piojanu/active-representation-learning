defaults:
  - agent: ppo
  - encoder: simclr
  - _self_

hydra:
  run:
    dir: ${_run.scratch_space}/${now:%Y%m%d}/${_run.exp_name}/${_run.run_name}
  sweep:
    dir: ${_run.scratch_space}/${now:%Y%m%d}/${_run.exp_name}
    subdir: ${_run.run_name}
  job:
    config:
      override_dirname:
        exclude_keys:
          - experiment
          - platform
          - seed

env: MiniWorld-PickupObjs-v0
rollout:
  bootstrap_value_at_time_limit: False
  force_non_episodic: True # Force the advantage calculation as if the env. was non-episodic
  gae_lambda: 0.95 # Generalized Advantage Estimation (GAE) lambda parameter (null to disable)
  gamma: 0.99 # Rewards discount factor
seed: 42
training:
  cuda: True
  num_processes: 8 # How many CPU processes to use for data collection
  num_steps: 2048 # Number of environment interactions before policy improvement
  total_steps: 3e6 # Total environment interactions throughout training

_run:
  exp_name: ${exp_name:}
  run_name: ${now:U%f},${hydra:job.override_dirname}
  scratch_space: ${oc.env:SCRATCH_SPACE}/active-representation-learning
