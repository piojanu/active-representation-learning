defaults:
  - agent: ppo
  - encoder: simclr
  - _self_
hydra:
  run:
    dir: ${oc.env:SCRATCH_SPACE}/active-representation-learning/${now:%Y%m%d}/${run_name:}/${now:T%H-%M-%S}
  sweep:
    dir: ${oc.env:SCRATCH_SPACE}/active-representation-learning/${now:%Y%m%d}/${run_name:}
    subdir: mr:${hydra.job.override_dirname}/s${seed}
  job:
    config:
      override_dirname:
        exclude_keys:
          - seed

env: 'MiniWorld-PickupObjs-v0'
logging:
  log_interval: 10 # Log every N policy iterations
  save_interval: 100 # Save every N policy iterations
rollout:
  bootstrap_value_at_time_limit: False
  force_non_episodic: True # Force the advantage calculation as if the env. was non-episodic
  gae_lambda: 0.95 # Generalized Advantage Estimation (GAE) lambda parameter (null to disable)
  gamma: 0.99 # Rewards discount factor
seed: 42
training:
  cuda: True
  num_processes: 8 # How many CPU processes to use for data collection
  num_steps: 2048 # Number of environment interactions before policy improvement
  total_steps: 2e6 # Total environment interactions throughout training
